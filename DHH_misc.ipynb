{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tweets by author gender\n",
    "#pip install gender_guesser\n",
    "\n",
    "import gender_guesser.detector as gender\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "session = requests.Session()\n",
    "session.auth = ('dhh19', 'heldig')\n",
    "files = requests.get(\"http://vm1973.kaj.pouta.csc.fi/brexit/data/continuous_rehydrated/all_tweets_df.msg\", auth=HTTPBasicAuth('dhh19', 'heldig'))\n",
    "total_df = pd.read_msgpack(files.content)\n",
    "\n",
    "d = gender.Detector()\n",
    "total_df[\"gender\"] = [d.get_gender(x.split()[0]) if x != \"\" else \"unknown\" for x in total_df[\"user_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tweets by MEPs\n",
    "\n",
    "labour = tuple(requests.get(\"https://raw.githubusercontent.com/stcoats/DHH19_Hackathon/master/labour_mps.txt\").text.split(\"\\n\"))\n",
    "tories = tuple(requests.get(\"https://raw.githubusercontent.com/stcoats/DHH19_Hackathon/master/tory_mps.txt\").text.split(\"\\n\"))\n",
    "snp = tuple(requests.get(\"https://raw.githubusercontent.com/stcoats/DHH19_Hackathon/master/snp_mps.txt\").text.split(\"\\n\"))\n",
    "libdems = tuple(requests.get(\"https://raw.githubusercontent.com/stcoats/DHH19_Hackathon/master/libdem_mps.txt\").text.split(\"\\n\"))\n",
    "labour_df = total_df[total_df[\"user_screen_name\"].isin((labour))]\n",
    "tories_df = total_df[total_df[\"user_screen_name\"].isin((tories))]\n",
    "snp_df = total_df[total_df[\"user_screen_name\"].isin((snp))]\n",
    "libdems_df = total_df[total_df[\"user_screen_name\"].isin((libdems))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frame of number of tweets in each language by user\n",
    "\n",
    "l_df = total_df.groupby([\"user_screen_name\",\"lang\"]).size().unstack().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the tweet texts, taking into account import possible emoji, japanese, or chinese text\n",
    "#Words are converted to lower case except usernames, hashtags, urls (in case you want to plot them) \n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import tinysegmenter\n",
    "segmenter = tinysegmenter.TinySegmenter()\n",
    "\n",
    "from chinese_tokenizer.tokenizer import Tokenizer\n",
    "jie_ba_tokenizer = Tokenizer().jie_ba_tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "def tknz(row):\n",
    "    line = row[\"text\"]\n",
    "    if row[\"lang\"]==\"ja\":\n",
    "        try:\n",
    "            tokens = segmenter.tokenize(line)\n",
    "            return tokens\n",
    "        except:\n",
    "            return 'error'\n",
    "    elif row[\"lang\"]==\"zh\":\n",
    "        try:\n",
    "            tokens = jie_ba_tokenizer(line)\n",
    "            return tokens\n",
    "        except:\n",
    "            return 'error'\n",
    "    else:\n",
    "        try:\n",
    "            tokens = [x.lower() if not x.startswith((\"@\",\"http\",\"www\",\"#\",\"t.co\")) else x for x in tokenizer.tokenize(line)]\n",
    "            return tokens\n",
    "        except:\n",
    "            return 'error'\n",
    "\n",
    "st_emojis= re.compile(\"[\\U0001f476\\U0001f9d2\\U0001f466\\U0001f467\\U0001f9d1\\U0001f468\\U0001f469\\U0001f9d3\\U0001f474\\U0001f475\\U0001f46e\\U0001f575\\U0001f482\\U0001f477\\U0001f934\\U0001f478\\U0001f473\\U0001f472\\U0001f9d5\\U0001f9d4\\U0001f471\\U0001f935\\U0001f470\\U0001f930\\U0001f931\\U0001f47c\\U0001f385\\U0001f936\\U0001f9d9\\U0001f9da\\U0001f9db\\U0001f9dc\\U0001f9dd\\U0001f64d\\U0001f64e\\U0001f645\\U0001f646\\U0001f481\\U0001f64b\\U0001f647\\U0001f926\\U0001f937\\U0001f486\\U0001f487\\U0001f6b6\\U0001f3c3\\U0001f483\\U0001f57a\\U0001f9d6\\U0001f9d7\\U0001f9d8\\U0001f6c0\\U0001f6cc\\U0001f574\\U0001f3c7\\U0001f3c2\\U0001f3cc\\U0001f3c4\\U0001f6a3\\U0001f3ca\\U000026f9\\U0001f3cb\\U0001f6b4\\U0001f6b5\\U0001f938\\U0001f93d\\U0001f93e\\U0001f939\\U0001f933\\U0001f4aa\\U0001f448\\U0001f449\\U0000261d\\U0001f446\\U0001f595\\U0001f447\\U0000270c\\U0001f91e\\U0001f596\\U0001f918\\U0001f919\\U0001f590\\U0000270b\\U0001f44c\\U0001f44d\\U0001f44e\\U0000270a\\U0001f44a\\U0001f91b\\U0001f91c\\U0001f91a\\U0001f44b\\U0001f91f\\U0000270d\\U0001f44f\\U0001f450\\U0001f64c\\U0001f932\\U0001f64f\\U0001f485\\U0001f442\\U0001f443]\")\n",
    "\n",
    "total_df[\"tokens\"]=total_df.apply(tknz,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make vectors of texts\n",
    "tweet_w2v = Word2Vec(size=400, min_count=10,window=5, workers=8)\n",
    "tweet_w2v.build_vocab([x for x in tqdm(total_df[\"tokens\"])])\n",
    "tweet_w2v.train([x for x in tqdm(total_df[\"tokens\"])], total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)\n",
    "\n",
    "total_df_f = total_df[total_df[\"gender\"] == \"female\"]\n",
    "total_df_m = total_df[total_df[\"gender\"] == \"male\"]\n",
    "\n",
    "# Vectors for females\n",
    "tweet_w2vF = Word2Vec(size=400, min_count=10,window=5, workers=8)\n",
    "tweet_w2vF.build_vocab([x for x in tqdm(total_df_f[\"tokens\"])])\n",
    "tweet_w2vF.train([x for x in tqdm(total_df_f(\"tokens\"))], total_examples=tweet_w2vF.corpus_count, epochs=tweet_w2vF.iter)\n",
    "\n",
    "# Vectors for males\n",
    "tweet_w2vM = Word2Vec(size=400, min_count=10,window=5, workers=8)\n",
    "tweet_w2vM.build_vocab([x for x in tqdm(total_df_m[\"tokens\"])])\n",
    "tweet_w2vM.train([x for x in tqdm(total_df_m(\"tokens\"))], total_examples=tweet_w2vM.corpus_count, epochs=tweet_w2vM.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blame = [\"blame\",\"disappointment\",\"disappointed\",\"dismay\",\"dismayed\",\"betray\",\"betrayal\",\"frustration\",\"embarrassment\"] #or whatever words you want\n",
    "n_items = tweet_w2v.wv.index2word[0:10000] # first 10k words\n",
    "\n",
    "# Getting a list of word vectors\n",
    "word_vectorsF = [tweet_w2vF[w] for w in n_items] + [tweet_w2vF[w] for w in blame if w in tweet_w2v.wv.vocab.keys()]\n",
    "\n",
    "# Convert from 400 to 2 dimensions \n",
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, perplexity = 50)\n",
    "tsne_w2vF = tsne_model.fit_transform(word_vectorsF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dfF = pd.DataFrame(tsne_w2vF, columns=['x', 'y'])\n",
    "tsne_dfF['words'] = n_items + [x for x in blame if x in tweet_w2vF.wv.vocab.keys()] \n",
    "tsne_dfF['freq'] = [tweet_w2vF.wv.vocab[w].count for w in n_items if w in tweet_w2vF.wv.vocab.keys()] + [tweet_w2vF.wv.vocab[w].count for w in blame if w in tweet_w2vF.wv.vocab.keys()]# + [tweet_w2vF.wv.vocab[w].count for w in nord_prof if w in tweet_w2vF.wv.vocab.keys()]\n",
    "tsne_dfF['color'] = [\"blue\"]*10000 + [\"red\"] * len([x for x in blame if x in tweet_w2vF.wv.vocab.keys()]) \n",
    "\n",
    "# Plotting \n",
    "plot_tfidfF = bp.figure(plot_width=700, plot_height=600, title=\"Map of 10000 word vectors and blame words for females\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_tfidfF.scatter(x='x', y='y', color = 'color', source=tsne_dfF)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"word\": \"@words\",\"freq\": \"@freq\"}\n",
    "show(plot_tfidf)\n",
    "\n",
    "#Do the same for the male words or for other social groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fragment iterates through the compressed .jsonl files in our \"continuous_rehydrated\" folder and makes a big data frame. \n",
    "# Much of it is borrowed from Twarc\n",
    "import sys\n",
    "from dateutil.parser import parse as date_parse\n",
    "from six import string_types\n",
    "\n",
    "def get_headings():\n",
    "    return [\n",
    "      'id',\n",
    "      'tweet_url',\n",
    "      'created_at',\n",
    "      'parsed_created_at',\n",
    "      'user_screen_name',\n",
    "      'text',\n",
    "      'tweet_type',\n",
    "      'coordinates',\n",
    "      'hashtags',\n",
    "      'retweeted_hashtags',\n",
    "      'media',\n",
    "      'urls',\n",
    "      'favorite_count',\n",
    "      'in_reply_to_screen_name',\n",
    "      'in_reply_to_status_id',\n",
    "      'in_reply_to_user_id',\n",
    "      'lang',\n",
    "      'place',\n",
    "      'possibly_sensitive',\n",
    "      'retweet_count',\n",
    "      'retweet_or_quote_id',\n",
    "      'retweet_or_quote_screen_name',\n",
    "      'retweet_or_quote_user_id',\n",
    "      'source',\n",
    "      'user_id',\n",
    "      'user_created_at',\n",
    "      'user_default_profile_image',\n",
    "      'user_description',\n",
    "      'user_favourites_count',\n",
    "      'user_followers_count',\n",
    "      'user_friends_count',\n",
    "      'user_listed_count',\n",
    "      'user_location',\n",
    "      'user_name',\n",
    "      'user_statuses_count',\n",
    "      'user_time_zone',\n",
    "      'user_urls',\n",
    "      'user_verified',\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_row(t, excel = False):\n",
    "    get = t.get\n",
    "    user = t.get('user').get\n",
    "    return [\n",
    "      get('id_str'),\n",
    "      tweet_url(t),\n",
    "      get('created_at'),\n",
    "      date_parse(get('created_at')),\n",
    "      user('screen_name'),\n",
    "      text(t) if not excel else clean_str(text(t)),\n",
    "      rt_text(t) if not excel else clean_str(rt_text(t)),\n",
    "      tweet_type(t),\n",
    "      coordinates(t),\n",
    "      hashtags(t),\n",
    "      retweeted_hashtags(t),\n",
    "      media(t),\n",
    "      urls(t),\n",
    "      get('favorite_count'),\n",
    "      get('in_reply_to_screen_name'),\n",
    "      get('in_reply_to_status_id'),\n",
    "      get('in_reply_to_user_id'),\n",
    "      get('lang'),\n",
    "      place(t),\n",
    "      get('possibly_sensitive'),\n",
    "      get('retweet_count'),\n",
    "      retweet_id(t),\n",
    "      retweet_screen_name(t),\n",
    "      retweet_user_id(t),\n",
    "      get('source'),\n",
    "      user('id_str'),\n",
    "      user('created_at'),\n",
    "      user('default_profile_image'),\n",
    "      user('description') if not excel else clean_str(user('description')),\n",
    "      user('favourites_count'),\n",
    "      user('followers_count'),\n",
    "      user('friends_count'),\n",
    "      user('listed_count'),\n",
    "      user('location') if not excel else clean_str(user('location')),\n",
    "      user('name') if not excel else clean_str(user('name')),\n",
    "      user('statuses_count'),\n",
    "      user('time_zone'),\n",
    "      user_urls(t),\n",
    "      user('verified'),\n",
    "    ]\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    if isinstance(string, string_types):\n",
    "        return string.replace('\\n', ' ').replace('\\r', '')\n",
    "    return None\n",
    "\n",
    "\n",
    "def text(t):\n",
    "    return t.get('full_text') or t.get('extended_tweet', {}).get('full_text') or t['text']\n",
    "\n",
    "def rt_text(t):\n",
    "    if \"retweeted_status\" in tweet:\n",
    "        return tweet[\"retweeted_status\"][\"full_text\"]\n",
    "\n",
    "def coordinates(t):\n",
    "    if 'coordinates' in t and t['coordinates']:\n",
    "        return '%f %f' % tuple(t['coordinates']['coordinates'])\n",
    "    return None\n",
    "\n",
    "\n",
    "def hashtags(t):\n",
    "    return ' '.join([h['text'] for h in t['entities']['hashtags']])\n",
    "\n",
    "def retweeted_hashtags(t):\n",
    "    if 'retweeted_status' in t and t['retweeted_status']:\n",
    "        return ' '.join([h['text'] for h in t['retweeted_status']['entities']['hashtags']])\n",
    "\n",
    "def media(t):\n",
    "    if 'extended_entities' in t and 'media' in t['extended_entities']:\n",
    "        return ' '.join([h['media_url_https'] for h in t['extended_entities']['media']])\n",
    "    elif 'media' in t['entities']: \n",
    "        return ' '.join([h['media_url_https'] for h in t['entities']['media']])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def urls(t):\n",
    "    return ' '.join([h['expanded_url'] or '' for h in t['entities']['urls']])\n",
    "\n",
    "\n",
    "def place(t):\n",
    "    if 'place' in t and t['place']:\n",
    "        return t['place']['full_name']\n",
    "\n",
    "\n",
    "def retweet_id(t):\n",
    "    if 'retweeted_status' in t and t['retweeted_status']:\n",
    "        return t['retweeted_status']['id_str']\n",
    "    elif 'quoted_status' in t and t['quoted_status']:\n",
    "        return t['quoted_status']['id_str']\n",
    "\n",
    "\n",
    "def retweet_screen_name(t):\n",
    "    if 'retweeted_status' in t and t['retweeted_status']:\n",
    "        return t['retweeted_status']['user']['screen_name']\n",
    "    elif 'quoted_status' in t and t['quoted_status']:\n",
    "        return t['quoted_status']['user']['screen_name']\n",
    "\n",
    "\n",
    "def retweet_user_id(t):\n",
    "    if 'retweeted_status' in t and t['retweeted_status']:\n",
    "        return t['retweeted_status']['user']['id_str']\n",
    "    elif 'quoted_status' in t and t['quoted_status']:\n",
    "        return t['quoted_status']['user']['id_str']\n",
    "\n",
    "\n",
    "def tweet_url(t):\n",
    "    return \"https://twitter.com/%s/status/%s\" % (t['user']['screen_name'], t['id_str'])\n",
    "\n",
    "\n",
    "def user_urls(t):\n",
    "    u = t.get('user')\n",
    "    if not u:\n",
    "        return None\n",
    "    urls = []\n",
    "    if 'entities' in u and 'url' in u['entities'] and 'urls' in u['entities']['url']:\n",
    "        for url in u['entities']['url']['urls']:\n",
    "            if url['expanded_url']:\n",
    "                urls.append(url['expanded_url'])\n",
    "    return ' '.join(urls)\n",
    "\n",
    "\n",
    "def tweet_type(t):\n",
    "    # Determine the type of a tweet\n",
    "    if t.get('in_reply_to_status_id'):\n",
    "        return 'reply'\n",
    "    if 'retweeted_status' in t:\n",
    "        return 'retweet'\n",
    "    if 'quoted_status' in t:\n",
    "        return 'quote'\n",
    "    return 'original'\n",
    "\n",
    "import json\n",
    "import glob\n",
    "files = glob.glob(\"/home/cloud-user/Hackathon/brexit/data/continuous_rehydrated/*.jsonl\") \n",
    "\n",
    "td = []\n",
    "for x in files:\n",
    "    with open(x) as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            td.append(get_row(tweet))\n",
    "            \n",
    "total_df = pd.DataFrame(td,columns = [\n",
    "      'id',\n",
    "      'tweet_url',\n",
    "      'created_at',\n",
    "      'parsed_created_at',\n",
    "      'user_screen_name',\n",
    "      'text',\n",
    "      'retweeted_text',\n",
    "      'tweet_type',\n",
    "      'coordinates',\n",
    "      'hashtags',\n",
    "      'retweeted_hashtags',\n",
    "      'media',\n",
    "      'urls',\n",
    "      'favorite_count',\n",
    "      'in_reply_to_screen_name',\n",
    "      'in_reply_to_status_id',\n",
    "      'in_reply_to_user_id',\n",
    "      'lang',\n",
    "      'place',\n",
    "      'possibly_sensitive',\n",
    "      'retweet_count',\n",
    "      'retweet_or_quote_id',\n",
    "      'retweet_or_quote_screen_name',\n",
    "      'retweet_or_quote_user_id',\n",
    "      'source',\n",
    "      'user_id',\n",
    "      'user_created_at',\n",
    "      'user_default_profile_image',\n",
    "      'user_description',\n",
    "      'user_favourites_count',\n",
    "      'user_followers_count',\n",
    "      'user_friends_count',\n",
    "      'user_listed_count',\n",
    "      'user_location',\n",
    "      'user_name',\n",
    "      'user_statuses_count',\n",
    "      'user_time_zone',\n",
    "      'user_urls',\n",
    "      'user_verified',\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
